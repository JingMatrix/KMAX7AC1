\documentclass[solutions]{exercices}

\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multicol,epsfig,csquotes}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{amsmath,amssymb,amsthm,enumitem,bbm,latexsym}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{listings}

%%%%%%%%%% environnements
%\theoremstyle{definition}
%\newtheorem{exo}{Exercice}


%%%%%%%%%% macros

\newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\Var}{\mathbb{V}\mathrm{ar}}


\begin{document}
{
\noindent {\sc M1 MAPI3  -  Simulations stochastiques \hfill 2025-2026}\\
Jianyu Ma \hfill \textit{jianyu.ma@math.univ-toulouse.fr}\\
Bastien Mallein \hfill \textit{bastien.mallein@math.univ-toulouse.fr}\\
Pierre Petit \hfill \textit{pierre.petit@math.univ-toulouse.fr}}


\vspace{2ex}

 \hrule
\begin{center}
\textbf{\large Contrôle partiel}
\vspace{2ex}
\end{center}
\hrule

\bigskip

On utilisera pour les exercices les quantiles approchés suivants de la loi normale centrée réduite.
\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
  $\alpha$ & 0.5 & 0.75 & 0.9 & 0.95 & 0.975 & 0.99 & 0.995\\
\hline
  $q_\alpha$ & 0 & 0.67 & 1.28 & 1.64 & 1.96 & 2.33 & 2.58\\
\hline
\end{tabular}
\end{figure}

\begin{exercice}[\textit{Lemme des réveils.}]
Soient $(X_1,X_2,X_3)$ trois variables aléatoires indépendantes de loi exponentielle de paramètre $1$, $2$ et $3$ respectivement.
\begin{enumerate}
  \item On note $Y = \min(X_1,X_2,X_3)$ et $I$ la variable aléatoire vérifiant $Y = X_I$. Démontrez que $Y$ et $I$ sont indépendantes, et précisez la loi de ces variables aléatoires.
  \item On note $J$ la variable aléatoire vérifiant $\max(X_1,X_2,X_3) = X_J$. Déterminer la loi jointe de $I$ et $J$.
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item
    \begin{itemize}
        \item \textbf{Loi de Y :} $Y = \min(X_1, X_2, X_3)$. Pour tout $t>0$,
        \[ \P(Y > t) = \P(X_1 > t, X_2 > t, X_3 > t). \]
        Par indépendance des $X_i$,
        \[ \P(Y > t) = \P(X_1 > t)\P(X_2 > t)\P(X_3 > t) = e^{-1 \cdot t} e^{-2t} e^{-3t} = e^{-6t}. \]
        On reconnaît la fonction de survie d'une loi exponentielle de paramètre $\lambda = 1+2+3=6$. Donc $Y \sim \mathcal{E}(6)$.

        \item \textbf{Loi de I :} $I$ est l'indice de la variable qui réalise le minimum. Pour $k \in \{1,2,3\}$,
        \[ \P(I=k) = \P(X_k < \min_{j \neq k} X_j). \]
        C'est un résultat classique sur la compétition de variables exponentielles indépendantes :
        \[ \P(I=k) = \frac{\lambda_k}{\sum_j \lambda_j} = \frac{k}{6}. \]
        Donc, $\P(I=1) = 1/6$, $\P(I=2) = 2/6$, $\P(I=3) = 3/6$.

        \item \textbf{Indépendance de Y et I :} On montre que $\P(Y > t, I=k) = \P(Y>t)\P(I=k)$.
        \begin{align*}
        \P(Y > t, I=k) &= \P(t < X_k < \min_{j \neq k} X_j) \\
        &= \int_t^\infty \P(x < \min_{j \neq k} X_j) f_{X_k}(x) \dd x \\
        &= \int_t^\infty \left( e^{-(\sum_{j \neq k} \lambda_j)x} \right) (\lambda_k e^{-\lambda_k x}) \dd x \quad \text{(par indépendance)}\\
        &= \lambda_k \int_t^\infty e^{-(\sum_j \lambda_j)x} \dd x \\
        &= \lambda_k \left[ \frac{e^{-(\sum_j \lambda_j)x}}{-\sum_j \lambda_j} \right]_t^\infty = \frac{\lambda_k}{\sum_j \lambda_j} e^{-(\sum_j \lambda_j)t} \\
        &= \P(I=k) \P(Y>t).
        \end{align*}
        $Y$ et $I$ sont donc indépendantes.
    \end{itemize}
    \item La loi jointe de $(I,J)$ est une loi sur $\{1,2,3\} \times \{1,2,3\}$.
    Si $i=j$, l'événement $I=i, J=i$ signifie $\min(X_1,X_2,X_3) = \max(X_1,X_2,X_3) = X_i$. Comme les lois sont continues, la probabilité que deux variables soient égales est nulle, donc $\P(X_1=X_2=X_3)=0$. Ainsi, $\P(I=j, J=j)=0$.

    Si $i \neq j$, l'événement $I=i, J=j$ signifie que $X_i$ est le minimum et $X_j$ est le maximum. Soit $k$ le troisième indice. Cela correspond à l'ordre $X_i < X_k < X_j$. La probabilité d'un ordre spécifique est donnée par :
    \[ \P(X_a < X_b < X_c) = \frac{\lambda_a}{\lambda_a+\lambda_b+\lambda_c} \times \frac{\lambda_b}{\lambda_b+\lambda_c}. \]
    Donc, pour $i \neq j$ et $k \notin \{i,j\}$:
    \[ \P(I=i, J=j) = \P(X_i < X_k < X_j) = \frac{\lambda_i}{\lambda_1+\lambda_2+\lambda_3} \times \frac{\lambda_k}{\lambda_k+\lambda_j}. \]
    Avec $\lambda_1=1, \lambda_2=2, \lambda_3=3$:
    \begin{itemize}
        \item $\P(I=1, J=2) = \frac{1}{6} \frac{3}{3+2} = \frac{1}{6} \frac{3}{5} = \frac{1}{10}$
        \item $\P(I=1, J=3) = \frac{1}{6} \frac{2}{2+3} = \frac{1}{6} \frac{2}{5} = \frac{1}{15}$
        \item $\P(I=2, J=1) = \frac{2}{6} \frac{3}{3+1} = \frac{2}{6} \frac{3}{4} = \frac{1}{4}$
        \item $\P(I=2, J=3) = \frac{2}{6} \frac{1}{1+3} = \frac{2}{6} \frac{1}{4} = \frac{1}{12}$
        \item $\P(I=3, J=1) = \frac{3}{6} \frac{2}{2+1} = \frac{3}{6} \frac{2}{3} = \frac{1}{3}$
        \item $\P(I=3, J=2) = \frac{3}{6} \frac{1}{1+2} = \frac{3}{6} \frac{1}{3} = \frac{1}{6}$
    \end{itemize}

\end{enumerate}
\end{solution}

\begin{exercice}[\textit{Chaîne de Markov.}]
Soit $(X_n, n \geq 1)$ le processus stochastique sur $\R$ défini récursivement par $X_0=0$ et
\[
  X_{n+1} = \frac{2X_n+Z_{n+1}}{3},
\]
où $(Z_{n}, n \geq 1)$ est une suite de variables aléatoires i.i.d. de loi normale centrée réduite.
\begin{enumerate}
  \item Montrer que $(X_n, n \geq 1)$ est une chaîne de Markov.
  \item Calculer $\E(X_n)$ et déterminer la formule de récurrence satisfaite par $\Var(X_n)$. En déduire la mesure invariante de la chaîne de Markov.
  \item Montrer que $\frac{1}{n}\sum_{i=1}^nX_i^2$ converge p.s. vers une limite qu'on déterminera.
  \item On suppose dans cette question que $X_0$ est distribuée selon la mesure invariante, calculer explicitement $\Cov(X_0,X_k)$ pour $k \geq 1$.
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item La loi de $X_{n+1}$ est déterminée par la valeur de $X_n$ et la variable aléatoire $Z_{n+1}$. La suite $(Z_n)$ étant i.i.d., $Z_{n+1}$ est indépendante du passé $(X_0, \dots, X_n)$. Par conséquent, la loi de $X_{n+1}$ conditionnellement à tout le passé $(X_0, \dots, X_n)$ ne dépend que de $X_n$. C'est la définition d'une chaîne de Markov.
    \item
    \begin{itemize}
        \item \textbf{Espérance :} Par linéarité de l'espérance, $\E(X_{n+1}) = \frac{2}{3}\E(X_n) + \frac{1}{3}\E(Z_{n+1})$. Comme $\E(Z_{n+1})=0$, on a $\E(X_{n+1}) = \frac{2}{3}\E(X_n)$. Sachant que $\E(X_0)=0$, on en déduit par récurrence que $\E(X_n)=0$ pour tout $n \ge 0$.
        \item \textbf{Variance :} $X_n$ est construit à partir de $Z_1, \dots, Z_n$, il est donc indépendant de $Z_{n+1}$.
        \[ \Var(X_{n+1}) = \Var\left(\frac{2}{3}X_n + \frac{1}{3}Z_{n+1}\right) = \left(\frac{2}{3}\right)^2\Var(X_n) + \left(\frac{1}{3}\right)^2\Var(Z_{n+1}). \]
        Comme $\Var(Z_{n+1})=1$, la relation de récurrence pour $v_n = \Var(X_n)$ est : $v_{n+1} = \frac{4}{9}v_n + \frac{1}{9}$.
        \item \textbf{Mesure invariante :} C'est une suite arithmético-géométrique qui converge vers un point fixe $v_\infty$ vérifiant $v_\infty = \frac{4}{9}v_\infty + \frac{1}{9}$. Cela donne $\frac{5}{9}v_\infty = \frac{1}{9}$, soit $v_\infty = 1/5$.
        Comme $X_n$ est une combinaison linéaire de variables gaussiennes, $X_n$ suit une loi normale. Puisque $\E(X_n)=0$ et $\Var(X_n) \to 1/5$, $X_n$ converge en loi vers $\mathcal{N}(0, 1/5)$. Cette loi limite est l'unique mesure de probabilité invariante de la chaîne.
    \end{itemize}
    \item La chaîne de Markov $(X_n)$ est irréductible et apériodique sur $\R$, et admet une mesure invariante $\pi = \mathcal{N}(0, 1/5)$. D'après le théorème ergodique pour les chaînes de Markov, pour une fonction $f$ telle que $\E_\pi[|f(X)|] < \infty$, on a :
    \[ \frac{1}{n}\sum_{i=1}^n f(X_i) \xrightarrow[n\to\infty]{p.s.} \E_\pi[f(X)]. \]
    Ici, on prend $f(x)=x^2$. On calcule l'espérance de $X^2$ sous la loi stationnaire :
    \[ \E_\pi[X^2] = \Var_\pi(X) + (\E_\pi[X])^2 = \frac{1}{5} + 0^2 = \frac{1}{5}. \]
    La limite est donc $1/5$.
    \item Si $X_0 \sim \mathcal{N}(0, 1/5)$, alors pour tout $n$, $X_n$ suit aussi cette loi. On a $\Cov(X_0, X_k) = \E(X_0 X_k) - \E(X_0)\E(X_k) = \E(X_0 X_k)$.
    En déroulant la récurrence :
    \[ X_k = \left(\frac{2}{3}\right)^k X_0 + \sum_{i=1}^k \left(\frac{2}{3}\right)^{k-i}\frac{Z_i}{3}. \]
    Multiplions par $X_0$ et prenons l'espérance :
    \[ \E(X_0 X_k) = \left(\frac{2}{3}\right)^k \E(X_0^2) + \E\left(X_0 \sum_{i=1}^k \left(\frac{2}{3}\right)^{k-i}\frac{Z_i}{3}\right). \]
    Comme $X_0$ (en régime stationnaire) est indépendant des "innovations" futures $Z_1, Z_2, \dots$, le second terme est nul.
    \[ \Cov(X_0, X_k) = \left(\frac{2}{3}\right)^k \E(X_0^2) = \left(\frac{2}{3}\right)^k \Var(X_0) = \frac{1}{5}\left(\frac{2}{3}\right)^k. \]
\end{enumerate}
\end{solution}

\begin{exercice}[\textit{Calcul approché d'intégrale.}]
On souhaite construire une estimation de l'intégrale
\[
  I = \int_0^\infty \frac{2x}{(1+x^2)^2} \exp(-3x^3) \sin(x) \dd x.
\]
\begin{enumerate}
  \item On note $X$ une variable aléatoire de densité $\frac{2 x}{(1+x^2)^2}\ind{x > 0}$ par rapport à la mesure de Lebesgue. Montrer que
  \[
    \P(X \leq x) = \frac{x^2}{1+x^2}.
  \]
  \item En déduire un algorithme permettant de simuler la loi de $X$ par méthode d'inversion de la fonction de répartition.
  \item On suppose que l'on dispose ici d'une suite de variables aléatoires $(X_n)_{n\ge1}$ i.i.d. de même loi que $X$. Proposer un estimateur de l'intégrale à partir de ces variables aléatoires qui converge presque sûrement vers $I$.
  \item Montrer une majoration de $\Var(\sin(X) e^{-3X^3})$. En déduire un intervalle de confiance asymptotique à 90\% pour $I$ obtenu par méthode de Monte-Carlo.
  \item On souhaite maintenant tirer une variable aléatoire $Y$ de densité $\frac{1}{Z}e^{-3x^3} \frac{2x}{(1+x^2)^2}\ind{x > 0}$ par rapport à la mesure de Lebesgue, où $Z$ est une variable aléatoire inconnue, mais pour laquelle on donnera une formule. Proposer un algorithme permettant de simuler cette variable aléatoire (on pourra montrer que $Z \leq 1$).
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item La fonction de répartition $F_X(x)$ est l'intégrale de la densité de 0 à $x$. Pour $x>0$ :
    \[ F_X(x) = \int_0^x \frac{2t}{(1+t^2)^2} \dd t. \]
    En posant le changement de variable $u = 1+t^2$, on a $du = 2t \dd t$. Les bornes deviennent $u(0)=1$ et $u(x)=1+x^2$.
    \[ F_X(x) = \int_1^{1+x^2} \frac{1}{u^2} \dd u = \left[-\frac{1}{u}\right]_1^{1+x^2} = -\frac{1}{1+x^2} - (-1) = 1 - \frac{1}{1+x^2} = \frac{x^2}{1+x^2}. \]
    \item On utilise la méthode d'inversion. On pose $u = F_X(x)$ et on résout pour $x$, avec $u \in [0,1]$.
    \[ u = \frac{x^2}{1+x^2} \implies u(1+x^2) = x^2 \implies u = x^2(1-u) \implies x^2 = \frac{u}{1-u}. \]
    Comme $x>0$, on a $x = \sqrt{\frac{u}{1-u}}$.
    L'algorithme est : 1. Générer $U \sim \mathcal{U}([0,1])$. 2. Calculer $X = \sqrt{\frac{U}{1-U}}$.
    \item L'intégrale $I$ peut s'écrire comme une espérance :
    \[ I = \int_0^\infty \left(\exp(-3x^3) \sin(x)\right) \left(\frac{2x}{(1+x^2)^2}\right) \dd x = \E_X[\exp(-3X^3) \sin(X)]. \]
    Par la loi forte des grands nombres, un estimateur convergent est la moyenne empirique :
    \[ \hat{I}_n = \frac{1}{n} \sum_{i=1}^n \exp(-3X_i^3) \sin(X_i). \]
    \item Soit $Y = \sin(X) e^{-3X^3}$. Pour tout $x \ge 0$, on a $|\sin(x)| \le 1$ et $e^{-3x^3} \le 1$. Donc $|Y| \le 1$.
    La variance est $\Var(Y) = \E(Y^2) - (\E(Y))^2$. On a $Y^2 \le 1$, donc $\E(Y^2) \le 1$. Par conséquent, une majoration simple est $\Var(Y) \le 1$.
    L'intervalle de confiance asymptotique à 90\% est donné par $[\hat{I}_n \pm q_{0.95} \frac{\sigma_Y}{\sqrt{n}}]$, où $\sigma_Y^2 = \Var(Y)$. D'après la table, $q_{0.95}=1.64$.
    En utilisant notre majoration $\sigma_Y \le 1$, un intervalle de confiance (conservateur) est :
    \[ \left[ \hat{I}_n - \frac{1.64}{\sqrt{n}}, \hat{I}_n + \frac{1.64}{\sqrt{n}} \right]. \]
    \item Il s'agit d'une simulation par la méthode du rejet.
    \begin{itemize}
        \item Loi cible : $f_Y(x) = \frac{1}{Z} e^{-3x^3} \frac{2x}{(1+x^2)^2} \propto e^{-3x^3} \frac{2x}{(1+x^2)^2}$.
        \item Loi de proposition : $g(x) = \frac{2x}{(1+x^2)^2}$, qui est la loi de $X$ que l'on sait simuler.
    \end{itemize}
    On cherche une constante $c$ telle que $f_Y(x) \le c \cdot g(x)$ pour tout $x$.
    \[ \frac{f_Y(x)}{g(x)} = \frac{\frac{1}{Z} e^{-3x^3} g(x)}{g(x)} = \frac{1}{Z}e^{-3x^3}. \]
    La fonction $x \mapsto e^{-3x^3}$ est décroissante sur $[0, \infty)$ et son maximum est 1 (atteint en $x=0$).
    On peut donc choisir $c = \frac{1}{Z}$. (Puisque $Z = \int_0^\infty e^{-3x^3} g(x) dx \le \int_0^\infty g(x) dx = 1$, on a $c \ge 1$).
    L'algorithme de rejet est le suivant :
    \begin{enumerate}
        \item Simuler un candidat $X$ selon la loi de proposition $g(x)$ (avec l'algorithme de la question 2).
        \item Simuler $U \sim \mathcal{U}([0,1])$.
        \item Accepter le candidat $X$ si $U \le \frac{f_Y(X)}{c \cdot g(X)} = \frac{\frac{1}{Z} e^{-3X^3} g(X)}{\frac{1}{Z} g(X)} = e^{-3X^3}$.
        \item Sinon, rejeter $X$ et retourner à l'étape 1.
    \end{enumerate}
    La constante inconnue $Z$ n'est pas nécessaire pour l'algorithme.
\end{enumerate}
\end{solution}

\begin{exercice}[\textit{Variables antithétiques.}]
Soit $f : [0,1] \to \R$ une fonction continue. On souhaite estimer la quantité
\[
  \mu := \int_0^1 f(u) \dd u
\]
par méthode de Monte-Carlo. Soit $(U_n, n \geq 1)$ une suite de variables aléatoires i.i.d. de loi uniforme sur $[0,1]$.

\begin{enumerate}
  \item On pose $\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n f(U_i)$. Montrer que $\hat{\mu}_n$ converge presque sûrement vers $\mu$ lorsque $n \to \infty$. On prendra soin de justifier l'application des théorèmes employés.
  \item Montrer que $\lim_{n \to \infty} \sqrt{n}(\hat{\mu}_n - \mu) = \mathcal{N}(0,\sigma^2)$ en loi, où on a posé
  \[
    \sigma^2 = \int_0^1 f(u)^2 \dd u - \left(\int_0^1 f(u) \dd u\right)^2.
  \]
  \item En utilisant que $S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (f(U_i) - \hat{\mu}_n)^2$ converge presque sûrement vers $\sigma^2$, donner un intervalle de confiance à $95\%$ pour $\mu$.
  \item On propose un nouvel estimateur de $\mu$ construit de la façon suivante :
  \[
    A_n := \frac{1}{n} \sum_{i=1}^n \frac{f(U_i) + f(1-U_i)}{2}.
  \]
  Montrer que $A_n$ converge presque sûrement vers $\mu$.
  \item Calculer la variance de $Z= \frac{f(U) + f(1-U)}{2}$ en fonction $\Var(f(U))$ et $\Cov(f(U),f(1-U))$.
  \item Sous quelles conditions a-t-on $\Var(A_n) \leq \Var(\hat{\mu}_n)$ ? Sous quelles conditions a-t-on $\Var(A_{n}) \leq \Var{\hat{\mu}_{2n}}$ ?
  \item Calculer explicitement $\Var(f(U))$ et $\Var(Z)$ pour $f : u \mapsto u^2$. Dans ce cas, est-il plus judicieux d'utiliser l'estimateur $A_n$ ou l'estimateur $\hat{\mu}_{2n}$ (on notera que dans chacun de ces deux cas, il faut réaliser $2n$ évaluations de la fonction $f$).
  \item On suppose $f$ croissante sur $[0,1]$. Soient $(U,U')$ deux variables indépendantes et identiquement distribuées de loi uniforme sur $[0,1]$.
  \begin{enumerate}
    \item Montrer que pour tout $u,v \in [0,1]$, $(f(u) - f(v))(f(1-u) - f(1-v)) \leq 0$.
    \item En déduire que $\E((f(U) - f(U'))(f(1-U) - f(1-U'))) \leq 0$.
    \item En conclure que $\Cov(f(U),f(1-U)) \leq 0$.
  \end{enumerate}
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item Les variables aléatoires $Y_i = f(U_i)$ sont i.i.d. Comme $f$ est continue sur $[0,1]$, elle est bornée, donc $\E[|f(U)|] < \infty$. La loi forte des grands nombres s'applique :
    \[ \hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n Y_i \xrightarrow[n\to\infty]{p.s.} \E[Y_1] = \E[f(U)] = \int_0^1 f(u) \cdot 1 \dd u = \mu. \]
    \item Les variables $f(U_i)$ sont i.i.d. avec une espérance $\mu$ et une variance $\sigma^2 = \Var(f(U))$. Comme $f$ est bornée, $\sigma^2$ est finie. Le Théorème Central Limite s'applique :
    \[ \sqrt{n}(\hat{\mu}_n - \mu) \xrightarrow[n\to\infty]{\mathcal{L}} \mathcal{N}(0,\sigma^2). \]
    La variance $\sigma^2$ est donnée par $\Var(f(U)) = \E[f(U)^2] - (\E[f(U)])^2 = \int_0^1 f(u)^2 \dd u - \mu^2$.
    \item Un intervalle de confiance asymptotique à 95\% pour $\mu$ est $[\hat{\mu}_n \pm q_{0.975} \frac{\sigma}{\sqrt{n}}]$.
    Comme $\sigma$ est inconnu, on l'estime par $S_n$. D'après la table, $q_{0.975} = 1.96$. L'intervalle de confiance est donc :
    \[ \left[ \hat{\mu}_n - 1.96 \frac{S_n}{\sqrt{n}}, \hat{\mu}_n + 1.96 \frac{S_n}{\sqrt{n}} \right]. \]
    \item Soit $Z_i = \frac{f(U_i) + f(1-U_i)}{2}$. Les $Z_i$ sont i.i.d. Calculons leur espérance :
    \[ \E[Z_i] = \frac{1}{2}(\E[f(U_i)] + \E[f(1-U_i)]). \]
    Si $U \sim \mathcal{U}([0,1])$, alors $1-U$ suit aussi une loi $\mathcal{U}([0,1])$. Donc $\E[f(U_i)]=\E[f(1-U_i)]=\mu$.
    \[ \E[Z_i] = \frac{1}{2}(\mu + \mu) = \mu. \]
    Par la loi forte des grands nombres, $A_n = \frac{1}{n}\sum Z_i$ converge p.s. vers $\E[Z_1] = \mu$.
    \item
    \begin{align*}
        \Var(Z) &= \Var\left(\frac{f(U) + f(1-U)}{2}\right) = \frac{1}{4} \Var(f(U) + f(1-U)) \\
        &= \frac{1}{4} \left( \Var(f(U)) + \Var(f(1-U)) + 2\Cov(f(U), f(1-U)) \right).
    \end{align*}
    Comme $U$ et $1-U$ ont même loi, $\Var(f(U)) = \Var(f(1-U))$.
    \[ \Var(Z) = \frac{1}{4} \left( 2\Var(f(U)) + 2\Cov(f(U), f(1-U)) \right) = \frac{1}{2}(\Var(f(U)) + \Cov(f(U), f(1-U))). \]
    \item On a $\Var(A_n) = \frac{\Var(Z)}{n}$ et $\Var(\hat{\mu}_n) = \frac{\Var(f(U))}{n}$.
    La condition $\Var(A_n) \le \Var(\hat{\mu}_n)$ est équivalente à $\Var(Z) \le \Var(f(U))$.
    \[ \frac{1}{2}(\Var(f(U)) + \Cov(f(U), f(1-U))) \le \Var(f(U)) \iff \Cov(f(U), f(1-U)) \le \Var(f(U)). \]
    Cette dernière inégalité est toujours vraie (par Cauchy-Schwarz, $|\text{Cov}| \le \text{Var}$). La vraie réduction de variance est obtenue si $\Cov < 0$.
    Pour la comparaison avec $\hat{\mu}_{2n}$, on a $\Var(A_n) \le \Var(\hat{\mu}_{2n})$ si $\frac{\Var(Z)}{n} \le \frac{\Var(f(U))}{2n}$, soit $2\Var(Z) \le \Var(f(U))$.
    \[ \Var(f(U)) + \Cov(f(U), f(1-U)) \le \Var(f(U)) \iff \Cov(f(U), f(1-U)) \le 0. \]
    \item Pour $f(u)=u^2$, $\mu=1/3$.
    $\Var(f(U)) = \E(U^4) - (\E(U^2))^2 = \frac{1}{5} - (\frac{1}{3})^2 = \frac{4}{45}$.
    $\Cov(f(U), f(1-U)) = \E(U^2(1-U)^2) - \E(U^2)\E((1-U)^2)$.
    $\E(U^2(1-2U+U^2)) = \E(U^2-2U^3+U^4) = \frac{1}{3} - \frac{2}{4} + \frac{1}{5} = \frac{1}{30}$.
    $\E((1-U)^2) = \E(U^2) = 1/3$.
    $\Cov = \frac{1}{30} - \frac{1}{3}\frac{1}{3} = \frac{1}{30} - \frac{1}{9} = \frac{3-10}{90} = -\frac{7}{90}$.
    La covariance est négative, donc $A_n$ est plus précis que $\hat{\mu}_{2n}$.
    Calculons les variances des estimateurs :
    $\Var(\hat{\mu}_{2n}) = \frac{\Var(f(U))}{2n} = \frac{4/45}{2n} = \frac{2}{45n}$.
    $\Var(Z) = \frac{1}{2}(\frac{4}{45} - \frac{7}{90}) = \frac{1}{2}(\frac{8-7}{90}) = \frac{1}{180}$.
    $\Var(A_n) = \frac{\Var(Z)}{n} = \frac{1}{180n}$.
    On a $\frac{1}{180n} < \frac{2}{45n} = \frac{8}{180n}$. La variance de $A_n$ est 8 fois plus faible que celle de $\hat{\mu}_{2n}$. Il est donc bien plus judicieux d'utiliser $A_n$.
    \item
    \begin{enumerate}
        \item Soient $u,v \in [0,1]$. Si $u \ge v$, alors $f(u) \ge f(v)$ (car $f$ est croissante), donc $f(u)-f(v) \ge 0$. De plus, $1-u \le 1-v$, donc $f(1-u) \le f(1-v)$, d'où $f(1-u)-f(1-v) \le 0$. Le produit des deux termes est donc négatif ou nul. Le raisonnement est identique si $u<v$.
        \item L'inégalité précédente est vraie pour toutes les réalisations $(u,v)$ du couple $(U,U')$. En prenant l'espérance, l'inégalité est conservée :
        \[ \E[(f(U) - f(U'))(f(1-U) - f(1-U'))] \le 0. \]
        \item On développe l'espérance. Comme $U,U'$ sont i.i.d. :
        \begin{align*}
        & \E[f(U)f(1-U) - f(U)f(1-U') - f(U')f(1-U) + f(U')f(1-U')] \le 0 \\
        & \E[f(U)f(1-U)] - \E[f(U)]\E[f(1-U')] - \E[f(U')]\E[f(1-U)] + \E[f(U')f(1-U')] \le 0
        \end{align*}
        Puisque $U, U', 1-U, 1-U'$ ont tous la même loi, les espérances sont égales :
        \begin{align*}
        & \E[f(U)f(1-U)] - \E[f(U)]\E[f(1-U)] - \E[f(U)]\E[f(1-U)] + \E[f(U)f(1-U)] \le 0 \\
        & 2\E[f(U)f(1-U)] - 2\E[f(U)]\E[f(1-U)] \le 0 \\
        & \Cov(f(U), f(1-U)) \le 0.
        \end{align*}
    \end{enumerate}
\end{enumerate}
\end{solution}

\end{document}
