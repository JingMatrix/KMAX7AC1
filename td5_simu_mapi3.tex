\documentclass[solutions]{exercices}

\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multicol,epsfig,csquotes}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{amsmath,amssymb,amsthm,enumitem,bbm,latexsym}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{listings}

%%%%%%%%%% environnements
%\theoremstyle{definition}
%\newtheorem{exo}{Exercice}


%%%%%%%%%% macros




\begin{document}
{
\noindent {\sc M1 MAPI3  -  Simulations stochastiques \hfill 2025-2026}\\
Jianyu Ma \hfill \textit{jianyu.ma@math.univ-toulouse.fr}\\
Bastien Mallein \hfill \textit{bastien.mallein@math.univ-toulouse.fr}\\
Pierre Petit \hfill \textit{pierre.petit@math.univ-toulouse.fr}}


\vspace{2ex}

 \hrule
\begin{center}
\textbf{\large TD 5 - Processus de Poisson et files d'attente}
\vspace{2ex}
\end{center}
\hrule

\bigskip
\textbf{TP-} Les exercices notés TP sont purement optionnels, et ne seront pas traités en classe.


\textbf{Définition :} \textit{Soit $X_n$ une suite de variables aléatoires indépendantes de loi exponentielle de paramètre $\lambda$. On définit  $S_0=0$ et $S_n=\sum_{i=1}^n X_i$ pour tout $n\ge1$.
Alors
$$N_t=\sum_{n\geq 1}1_{\{S_n\leq t\}}=\max \{n\text{ tels que } S_{n}\leq t\}$$
définit un processus de Poisson d'intensité $\lambda$.}

Ainsi, $(N_t)_{t\ge0}$ est une fonction aléatoire croissante, constante par morceaux qui compte le nombre d'événements survenus avant l'instant $t$ lorsque ces événements surviennent à des dates séparées par des durées indépendantes exponentielles de paramètre $\lambda$.

\begin{exercice}[ \textbf{TP -} Simulation d'un processus de Poisson]
\begin{enumerate}
\item \textbf{Simulation des $n$ premiers sauts.} Soit $n>1$, proposer un algorithme qui renvoie les $n$ premiers instants de saut d'un processus de Poisson.
\item En utilisant le programme précédent, tracer un histogramme de la loi du $n^{eme}$ saut.
\item \textbf{Simulation jusqu'à un instant $t$ fixé.}
Soit $t>0$ fixé. Pour simuler $N_t$, il faut donc tirer des exponentielles jusqu'à ce que $S_{n+1}> t$. \'Ecrire l'algorithme qui stocke les instants de sauts $S_1,S_2,\ldots$ du processus entre $0$ et $t$ et qui renvoie la valeur terminale $N_t$.
\item Vérifier à l'aide d'un histogramme que pour $t\ge0$ fixé, la variable $N_t$ suit une loi de Poisson de paramètre $\lambda t$.
\item Tracer une trajectoire du processus sur $[0,10]$. On pourra par exemple utiliser la fonction \texttt{matplotlib.pyplot.step}.
\item Illustrer par un algorithme la convergence presque sure de $(N_t/t)$vers $\lambda$ lorsque $t$ tend vers l'infini.
\end{enumerate}
\end{exercice}

\begin{solution}
Cet exercice est purement pratique. Les solutions correspondent à des algorithmes à implémenter en Python.
\begin{enumerate}
    \item L'algorithme est une application directe de la définition :
    \begin{itemize}
        \item Simuler $n$ variables i.i.d. $X_1, \dots, X_n$ de loi $\mathcal{E}(\lambda)$.
        \item Calculer les sommes cumulées $S_k = \sum_{i=1}^k X_i$ pour $k=1, \dots, n$.
        \item Renvoyer le vecteur $(S_1, \dots, S_n)$.
    \end{itemize}
    \item Théoriquement, $S_n = \sum_{i=1}^n X_i$ est la somme de $n$ variables exponentielles i.i.d. de paramètre $\lambda$. Cette somme suit une loi Gamma (ou d'Erlang) de paramètres $(n, \lambda)$. L'histogramme des $S_n$ simulés devrait correspondre à la densité de cette loi.
    \item L'algorithme est le suivant :
	\begin{itemize}
		\item Initialiser une liste de sauts \texttt{sauts = []} et un temps courant \texttt{temps\_total = 0}.
		\item \textbf{Tant que} \texttt{temps\_total <= t} :
        \begin{itemize}
            \item Simuler une variable $X \sim \mathcal{E}(\lambda)$.
			\item Mettre à jour \texttt{temps\_total = temps\_total + X}.
			\item \textbf{Si} \texttt{temps\_total <= t}, ajouter \texttt{temps\_total} à la liste \texttt{sauts}.
        \end{itemize}
        \item Renvoyer la liste `sauts` et sa longueur `len(sauts)`, qui est la valeur de $N_t$.
    \end{itemize}
    \item Pour vérifier cela, on répète l'algorithme précédent un grand nombre de fois pour obtenir un échantillon de valeurs de $N_t$. L'histogramme de cet échantillon doit être comparé à la fonction de masse d'une loi de Poisson de paramètre $\lambda t$.
    \item Pour tracer une trajectoire, on utilise la liste des temps de sauts $[S_1, \dots, S_k]$ obtenue. Les points à tracer sont $(0,0), (S_1,0), (S_1,1), (S_2,1), (S_2,2), \dots$. La fonction `step` de Matplotlib est idéale pour cela.
    \item On simule une trajectoire sur un très long intervalle de temps $[0, T]$. On calcule le processus $Y_t = N_t / t$ pour des valeurs de $t$ croissantes le long de la trajectoire. En traçant $Y_t$ en fonction de $t$, on devrait observer une courbe qui se stabilise vers la valeur $\lambda$.
\end{enumerate}
\end{solution}

\begin{exercice}[Superposition de processus de Poisson]
On s'intéresse dans cet exercice à une propriété intéressante des processus de Poisson :\\
\textit{Si $(M^1_t)_{t\ge0}$ et $(M^2_t)_{t\ge0}$ sont deux processus de Poisson indépendants d'intensité $\lambda_1$ et $\lambda_2$, alors  le processus $(N_t)_{t\ge0}=(M^1_t+M^2_t)_{t\ge0}$ est aussi un processus de Poisson de paramètre $\lambda_1+\lambda_2$.}
\begin{enumerate}
\item Soit $t>0$ fixé, déterminer la loi de $M^1_t$ et $M^2_t$. En déduire la loi de $N_t$ ?
\item On définit le premier saut d'un processus de Poisson comme le premier instant où le processus vaut $1$. Par construction, ce premier saut est une variable aléatoire exponentielle de paramètre $\lambda$. On notera $U^1$ le premier saut de $(M^1_t)_{t\ge0}$ et $U^2$ celui de $(M^2_t)_{t\ge0}$. \\
Donner la loi du premier instant de saut de $(N_t)_{t\ge0}$.
\item Calculer la probabilité que ce premier instant de saut corresponde au premier instant de saut de $(M^1_t)_{t\ge0}$.
\item \textbf{TP -} Inversement, si $(N_t)_{t\ge0}$ est un processus de Poisson d'intensité $\lambda$, on construit les processus $(M^1_t)_{t\ge0}$ et $(M^2_t)_{t\ge0}$ comme suit : ils partent de $0$, sont constants par morceau, et à chaque saut de $(N_t)_{t\ge0}$, on fait sauter $(M^1_t)_{t\ge0}$ avec probabilité $p$, sinon c'est $(M^2_t)_{t\ge0}$ qui saute, les tirages étant indépendants à chaque saut de $(N_t)_{t\ge0}$. \textit{Alors $(M^1_t)_{t\ge0}$ et $(M^2_t)_{t\ge0}$ sont deux processus de Poisson indépendants.}\\
\'Ecrire un algorithme qui simule $(N_t)_{t\ge0}$, $(M^1_t)_{t\ge0}$ et $(M^2_t)_{t\ge0}$ en même temps. A l'aide de ces simulations retrouver les paramètres de $(M^1_t)_{t\ge0}$ et $(M^2_t)_{t\ge0}$ en fonction de $\lambda$ et $p$.
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item Pour un $t$ fixé, $M^1_t$ suit une loi de Poisson de paramètre $\lambda_1 t$, et $M^2_t$ suit une loi de Poisson de paramètre $\lambda_2 t$. Comme les processus sont indépendants, les variables aléatoires $M^1_t$ et $M^2_t$ le sont aussi. La somme de deux variables de Poisson indépendantes est une variable de Poisson dont le paramètre est la somme des paramètres.
    Ainsi, $N_t = M^1_t + M^2_t$ suit une loi de Poisson de paramètre $(\lambda_1 t + \lambda_2 t) = (\lambda_1 + \lambda_2)t$.
    Cette propriété est nécessaire pour que $N_t$ soit un processus de Poisson de paramètre $\lambda_1+\lambda_2$, mais pas suffisante (il faut aussi vérifier les accroissements indépendants).
    \item Le premier instant de saut de $N_t$ est le premier instant où un événement se produit, soit dans le processus 1, soit dans le processus 2. Cet instant est donc $U = \min(U^1, U^2)$.
    On a $U^1 \sim \mathcal{E}(\lambda_1)$ et $U^2 \sim \mathcal{E}(\lambda_2)$ indépendantes. D'après une propriété classique, le minimum de deux variables exponentielles indépendantes suit une loi exponentielle dont le paramètre est la somme des paramètres. Donc, $U \sim \mathcal{E}(\lambda_1+\lambda_2)$.
    \item Il s'agit de calculer $\P(U=U^1)$, ce qui équivaut à $\P(U^1 < U^2)$. C'est la probabilité que le processus 1 "gagne la course". Cette probabilité vaut $\frac{\lambda_1}{\lambda_1+\lambda_2}$.
    \item \textbf{TP-} L'algorithme de "thinning" (ou éclaircissage) est le suivant :
    \begin{itemize}
        \item Simuler les instants de saut $S_1, S_2, \dots$ d'un processus de Poisson $N_t$ de paramètre $\lambda$.
        \item Pour chaque saut $S_i$ :
        \begin{itemize}
            \item Tirer une variable $U \sim \mathcal{U}([0,1])$.
            \item Si $U < p$, assigner ce saut au processus $M^1_t$.
            \item Sinon, l'assigner au processus $M^2_t$.
        \end{itemize}
        \item Reconstruire les trajectoires de $M^1_t$ et $M^2_t$ à partir des listes de sauts respectives.
    \end{itemize}
    Pour retrouver les paramètres, on simule sur un temps long $T$. On calcule $M^1_T$ et $M^2_T$. Les estimations des paramètres sont $\hat{\lambda}_1 = M^1_T / T$ et $\hat{\lambda}_2 = M^2_T / T$. On devrait observer que $\hat{\lambda}_1 \approx \lambda p$ et $\hat{\lambda}_2 \approx \lambda (1-p)$.
\end{enumerate}
\end{solution}

\begin{exercice}[Paradoxe des autobus]
Des autobus arrivent à une station suivant un processus $(N_t)_{t\ge0}$ de Poisson de paramètre $4$ (l'unité de temps étant l'heure). On note $T_1\le T_2\le ...$ la suite des instants de passage des bus.
\begin{enumerate}
\item Combien d'autobus passe-t-il en moyenne en une heure ?
\item On suppose que l'on arrive à l'instant $a>0$ à l'arrêt de bus. Le prochain bus passera à un instant $T_K$ où $K$ est un entier aléatoire tel que $T_{K-1}<a \le T_K$.
On note $D=T_K-a$ la durée d'attente avant le prochain autobus. Calculer la loi jointe de $D$ et $a-T_{K-1}$.
\item En déduire le temps d'attente moyen $\mathbb{E}(D)$.
\end{enumerate}
\end{exercice}

\begin{solution}
Cet exercice peut être résolu de manière entièrement autonome en utilisant uniquement la définition du processus de comptage $(N_t)$ par les temps inter-arrivées exponentiels.

\begin{enumerate}
    \item
    Soit $(X_i)_{i \ge 1}$ la suite des temps i.i.d. entre les arrivées des bus, avec $X_i \sim \mathcal{E}(\lambda=4)$. Le temps moyen entre deux bus consécutifs est $\E[X_i] = 1/\lambda = 1/4$ d'heure. Le nombre moyen de bus passant en une heure est l'inverse de cette durée, soit $\lambda = 4$.

    \item (Cette question est une introduction aux suivantes.)

    \item
    On cherche la loi jointe de $D = T_K - a$ et $B = a - T_{K-1}$. La méthode la plus directe consiste à calculer leur fonction de survie jointe $\P(D > x, B > y)$.

    \textbf{Étape 1 : Formuler l'événement d'une manière nouvelle.}
    L'événement $\{D > x, B > y\}$ signifie qu'il n'y a aucun instant de saut $T_j$ dans l'intervalle $[a-y, a+x]$.

    Considérons le processus à partir de l'instant $s = a-y$. L'événement ci-dessus est équivalent à dire que le \textit{premier bus à arriver après l'instant $s$} arrivera après un temps de $x+y$.

    Soit $D_s$ le temps d'attente pour le premier événement après un instant quelconque $s$. L'événement est donc $\{D_{a-y} > x+y\}$. Si nous pouvons montrer que $D_s \sim \mathcal{E}(\lambda)$ pour tout $s$, le problème est résolu.

    \textbf{Étape 2 : Démontrer la loi du temps d'attente résiduel (un lemme clé).}

    Prouvons que pour tout $s > 0$, la variable aléatoire $D_s$ (le temps d'attente jusqu'au prochain événement à partir de $s$) suit une loi $\mathcal{E}(\lambda)$.

    Soit $N_s = k$ le nombre d'événements jusqu'à l'instant $s$. Par définition, cela signifie que $T_k \le s < T_{k+1}$.
    Le temps d'attente $D_s$ est $T_{k+1} - s$. On sait que $T_{k+1} = T_k + X_{k+1}$, où $X_{k+1} \sim \mathcal{E}(\lambda)$ est indépendant de $T_k$.

    Nous voulons calculer $\P(D_s > t)$ pour un $t>0$. Pour ce faire, nous conditionnons sur l'histoire du processus jusqu'à l'instant $s$, qui se résume à connaître $k$ et la valeur de $T_k$. Sachant que $T_k \le s$, nous savons que l'intervalle $X_{k+1}$ a déjà "survécu" pendant au moins un temps $s-T_k$.
    \begin{align*}
      \P(D_s > t \mid \text{histoire jusqu'à } s) &= \P(T_{k+1} - s > t \mid T_k, N_s=k) \\
      &= \P(T_k + X_{k+1} - s > t \mid T_k) \\
      &= \P(X_{k+1} > s - T_k + t \mid T_k).
    \end{align*}
    L'information que nous avons sur $X_{k+1}$ est $X_{k+1} > s - T_k$. Par la \textbf{propriété d'absence de mémoire} de la loi exponentielle, qui stipule que $\P(X > u+v \mid X>u) = \P(X>v)$, on a :
    \[ \P(X_{k+1} > (s - T_k) + t \mid X_{k+1} > s-T_k) = \P(X_{k+1} > t) = e^{-\lambda t}. \]
    Cette probabilité conditionnelle est $e^{-\lambda t}$ et, de manière cruciale, elle ne dépend pas de l'histoire (c'est-à-dire de $k$ ou de $T_k$). Puisque la probabilité conditionnelle est constante, la probabilité inconditionnelle est la même.

    Nous avons donc prouvé que, pour tout $s>0$, le temps d'attente résiduel $D_s$ suit une loi $\mathcal{E}(\lambda)$.

    \textbf{Étape 3 : Appliquer le lemme et conclure.}

    Revenons à notre problème initial. Nous avons montré que l'événement $\{D > x, B > y\}$ est équivalent à $\{D_{a-y} > x+y\}$.
    En utilisant le lemme que nous venons de démontrer avec $s = a-y$ et un temps d'attente $t = x+y$ :
    \[ \P(D > x, B > y) = \P(D_{a-y} > x+y) = e^{-\lambda(x+y)}. \]

    La fonction de survie jointe se factorise :
    \[ \P(D > x, B > y) = e^{-\lambda x} \cdot e^{-\lambda y}. \]
    Cette factorisation prouve que $D$ et $B$ sont indépendantes, et que chacune suit une loi exponentielle de paramètre $\lambda$.

    \textbf{Conclusion :} La loi jointe de $(D, a-T_{K-1})$ est celle de deux variables aléatoires \textbf{indépendantes}, suivant chacune une \textbf{loi exponentielle de paramètre $\lambda=4$}.

    \item
    Le temps d'attente moyen est $\mathbb{E}(D)$. Puisque nous avons établi rigoureusement que $D$ suit une loi exponentielle de paramètre $\lambda=4$, son espérance est :
    \[ \mathbb{E}(D) = \frac{1}{\lambda} = \frac{1}{4} \text{ heure} = 15 \text{ minutes}. \]

    \textbf{Explication du paradoxe :} Le résultat est contre-intuitif. Le temps moyen \textit{entre} deux bus consécutifs est $\mathbb{E}(X_n) = 1/\lambda = 15$ minutes. Pourtant, si on arrive à un instant aléatoire, le temps moyen qu'il nous \textit{reste} à attendre est aussi de 15 minutes.

    Le "paradoxe" se résout en comprenant qu'un observateur arrivant à un instant aléatoire a plus de chances de tomber dans un intervalle inter-arrivées qui est \textit{plus long que la moyenne}. C'est le \textit{biais de l'échantillonnage par intervalle}. Les longs intervalles occupent plus de place sur l'axe du temps, et sont donc plus susceptibles d'être "choisis" par une arrivée aléatoire. La durée de l'intervalle spécifique dans lequel on arrive, $X_K = B+D$, ne suit pas une loi $\mathcal{E}(\lambda)$. Sa durée moyenne est en fait $2/\lambda$. Comme notre attente moyenne est de $1/\lambda$, et que par symétrie le temps déjà écoulé $B$ a aussi une moyenne de $1/\lambda$, la durée moyenne de l'intervalle "choisi" est bien $\E[B+D] = \E[B]+\E[D] = 1/\lambda + 1/\lambda = 2/\lambda$.
    Notre résultat est donc parfaitement cohérent avec ce phénomène.

\end{enumerate}
\end{solution}

\begin{exercice}[Comportement en temps long]
Soit $(N_t)_{t\ge0}$ un processus de Poisson de paramètre $\lambda>0$. L'objectif de l'exercice est de montrer que $$\frac{N_t}{t} \underset{t\to\infty}{\longrightarrow} \lambda\qquad p.s. .$$

\begin{enumerate}
\item On regarde tout d'abord le processus à des temps entiers.
Justifier que pour tout $n\in \N$,
$$N_n=\sum_{k=1}^n N_k-N_{k-1}.$$
\item Appliquer la loi forte des grands nombres à la somme précédente.
\item Justifier que pour tout $t\in[n,n+1]$,
	$$\frac{N_n}{n}\frac{n}{n+1}\le \frac{N_t}{t} \le \frac{N_{n+1}}{n+1}\frac{n+1}{n}$$
\item Conclure
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item La somme $\sum_{k=1}^n (N_k-N_{k-1})$ est une somme télescopique :
    \[ \sum_{k=1}^n (N_k-N_{k-1}) = (N_1-N_0) + (N_2-N_1) + \dots + (N_n-N_{n-1}). \]
    Les termes s'annulent deux à deux, et il reste $N_n - N_0$. Comme $N_0=0$ par définition, on a bien $N_n=\sum_{k=1}^n (N_k-N_{k-1})$.
    \item Les accroissements d'un processus de Poisson sont indépendants. Les variables $Y_k = N_k - N_{k-1}$ sont donc indépendantes. De plus, les accroissements sont stationnaires : la loi de $N_t - N_s$ ne dépend que de $t-s$. Ici, $k-(k-1)=1$, donc toutes les variables $Y_k$ suivent la même loi. Cette loi est le nombre d'événements dans un intervalle de temps de longueur 1, c'est-à-dire une loi de Poisson de paramètre $\lambda \times 1 = \lambda$.
    Les $Y_k$ sont i.i.d. d'espérance $\E(Y_k) = \lambda$. D'après la loi forte des grands nombres :
    \[ \frac{N_n}{n} = \frac{1}{n}\sum_{k=1}^n Y_k \xrightarrow[n\to\infty]{p.s.} \E(Y_1) = \lambda. \]
    \item Une inégalité plus simple et directe vient de la croissance de $N_t$.
    Pour tout $t \ge 0$, soit $n = \lfloor t \rfloor$. On a alors $n \le t \le n+1$.
    Comme $N_t$ est une fonction croissante, on a $N_n \le N_t \le N_{n+1}$.
    En divisant par $t$, on obtient :
    \[ \frac{N_n}{t} \le \frac{N_t}{t} \le \frac{N_{n+1}}{t}. \]
    On peut ensuite encadrer $1/t$ par $1/(n+1) \le 1/t \le 1/n$. Cela donne :
    \[ \frac{N_n}{n+1} \le \frac{N_t}{t} \le \frac{N_{n+1}}{n}. \]
    On peut réécrire cet encadrement comme :
    \[ \frac{N_n}{n} \frac{n}{n+1} \le \frac{N_t}{t} \le \frac{N_{n+1}}{n+1} \frac{n+1}{n}. \]
    \item On fait tendre $t \to \infty$, ce qui implique $n = \lfloor t \rfloor \to \infty$.
    Dans l'encadrement, on a :
    \begin{itemize}
        \item $\lim_{n\to\infty} \frac{N_n}{n} = \lambda$ p.s. (d'après la question 2).
        \item $\lim_{n\to\infty} \frac{n}{n+1} = 1$.
        \item $\lim_{n\to\infty} \frac{N_{n+1}}{n+1} = \lambda$ p.s. (c'est la même limite que ci-dessus).
        \item $\lim_{n\to\infty} \frac{n+1}{n} = 1$.
    \end{itemize}
    Le terme de gauche et le terme de droite de l'inégalité convergent donc tous les deux presque sûrement vers $\lambda \times 1 = \lambda$.
    Par le théorème des gendarmes, on conclut que :
    \[ \lim_{t\to\infty} \frac{N_t}{t} = \lambda \quad p.s. \]
\end{enumerate}
\end{solution}

\begin{exercice}[Files d'attente : théorie et pratique]
On veut modéliser une file d'attente devant un unique guichet pour lequel les clients arrivent suivant un processus de Poisson de paramètre $\lambda>0$ : c'est à dire que les arrivées des clients successifs sont espacées de variables aléatoires exponentielles indépendantes de paramètre $\lambda>0$.\\
Les temps de services des clients sont indépendants entre eux et indépendants des temps d'arrivées des clients et suivent une loi exponentielle de paramètres $\mu$. On note $(W_t)_{t\ge0}$ le processus qui compte le nombre de personnes présentes dans la file (avec la convention qu'une personne au guichet en train d'être servie compte comme étant dans la file).
$(W_t)_{t\ge0}$  est un processus de saut à valeurs dans $\N$ qui augment de $1$ lorsqu'un nouveau client arrive et diminue de $1$ lorsqu'un client à été servi.

On note $T_n$ la suite des temps de sauts (arrivée ou départs des clients) avec $T_0=0$ et $Z_n=W_{T_n}$ le nombre de clients à cet instant de saut. Remarquons que la donnée de $(T_n, Z_n)_{n\ge0}$ permet de reconstruire la file d'attente en temps continu.

Dans la suite on admettra le résultat suivant :\\
\textbf{Proposition} {\it Si $X\sim\mathcal{E}(\lambda)$ et $Y\sim  \mathcal{E}(\mu)$ sont indépendantes. Alors $V=\min(X,Y)\sim\mathcal{E}(\lambda+\mu)$ et $W=\mathbf{1}_{V=X}\sim\mathcal{B}er(\frac{\lambda}{\lambda+\mu})$ et $V$ et $W$ sont indépendantes}.

\begin{enumerate}
\item Déduire de la proposition les affirmations suivantes :\begin{itemize}
\item Si $Z_n=0$ alors $T_{n+1}-T_n\sim \mathcal{E}(\lambda)$ et $Z_{n+1}=1$
\item Si $Z_n>0$ alors $T_{n+1}-T_n\sim \mathcal{E}(\lambda+\mu)$ et $Z_{n+1}=Z_n+Y_n$ où $Y_n$ est une variable aléatoire à valeurs dans $\{-1,1\}$ telle que $\mathbb{P}(Y_n=1)=\frac{\lambda}{\lambda+\mu}$.
\end{itemize}
\item On s'intéresse maintenant uniquement à la chaine de Markov $(Z_n)_{n\ge0}$. Décrire ses probabilités de transitions.
\item On note $p_n$ la probabilité que $Z$ atteigne $0$ partant de $n$. En conditionnant par rapport au premier évènement, montrer que $p_n$ est solution du système d'équations suivant :
\begin{align*}
&p_n=\frac{\lambda}{\lambda+\mu}p_{n+1} + \frac{\mu}{\lambda+\mu}p_{n-1}
&p_0=1
\end{align*}
\item Résoudre cette équation de récurrence en écrivant une relation entre $p_{n+1}-p_n$ et $p_n-p_{n-1}$. En déduire que si $\lambda<\mu$, alors $p_1=1$.
\item \textbf{TP -} \'Ecrire une fonction qui prend en paramètre $\lambda, \mu$ et $t$, et simule la file d'attente sur l'intervalle $[0,t]$. Ce programme renvoie une matrice avec les temps de sauts et le nombre de client dans la file d'attente.
\item \textbf{TP -} Observez que la file d'attente semble avoir des comportements différents selon que $\lambda>\mu$, $\lambda<\mu$ et $\lambda=\mu$.
\item \textbf{TP -} \underline{Cas $\lambda>\mu$ :} Représenter graphiquement $(Z_n/T_n)_{n\ge0}$ en fonction de $T_n$. En faisant varier successivement les valeurs de $\lambda$ et $\mu$ proposer une expression de la limite presque sure $a(\lambda, \mu)$ de $\frac{W_t}{t}$ quand $t\to\infty$.
\item \textbf{TP -}\underline{Cas $\lambda<\mu$ : } Illustrer le fait que lorsque $t\to\infty$ la loi du processus $W_t$ converge vers une loi stationnaire $\pi$ définie par $$\pi(\{k\})=\left(\frac\lambda\mu\right)^k\left(1-\frac\lambda\mu\right).$$
\textit{Attention : il faut vraiment considérer le processus en temps continu pour prendre en compte le fait qu'on peut rester plus longtemps dans certains états.}
\item  \textbf{TP -} Calcul du temps d'inactivité du serveur : On considère ici $\lambda<\mu$. On appelle temps d'inactivité du serveur, les durées pendant lesquelles la file est vide. En simulant la file d'attente sur un temps long, obtenir un échantillon de $1000$ temps d'inactivité du serveur. Donner un estimation du temps moyen d'inactivité. Ceci vous parait-il cohérent ?
\end{enumerate}
\end{exercice}

\begin{solution}
L'essentiel de cet exercice est traité dans l'exercice 4 du TD4, mais il est abordé ici sous l'angle du processus de Poisson.
\begin{enumerate}
    \item La proposition est la clé de la simulation (algorithme de Gillespie). Si la file est vide ($Z_n=0$), le seul événement possible est une arrivée. Le temps d'attente pour cet événement, qui est l'intervalle entre la fin du service précédent et l'arrivée du client suivant, suit une loi $\mathcal{E}(\lambda)$. Si la file n'est pas vide ($Z_n>0$), deux événements peuvent se produire : une arrivée (au taux $\lambda$) ou un départ (au taux $\mu$). Le prochain événement est le minimum des deux temps d'attente, qui suit $\mathcal{E}(\lambda+\mu)$, et la nature de cet événement (arrivée ou départ) est une Bernoulli de paramètre $\frac{\lambda}{\lambda+\mu}$.
    \item La chaîne $(Z_n)$ est la chaîne de sauts du processus en temps continu. C'est une marche aléatoire sur $\mathbb{N}$.
    \begin{itemize}
        \item Depuis l'état 0, on saute toujours vers 1 : $P_{0,1}=1$.
        \item Depuis l'état $n > 0$, on saute vers $n+1$ avec probabilité $\frac{\lambda}{\lambda+\mu}$ et vers $n-1$ avec probabilité $\frac{\mu}{\lambda+\mu}$.
    \end{itemize}
    \item C'est un problème de ruine du joueur classique. On cherche la probabilité d'atteindre l'état 0. Soit $p_n$ cette probabilité partant de $n$. On a $p_0=1$. Pour $n>0$, en conditionnant sur le premier saut :
    \[ p_n = \P(\text{atteindre } 0) = \P(\text{saut en } n+1)p_{n+1} + \P(\text{saut en } n-1)p_{n-1} = \frac{\lambda}{\lambda+\mu}p_{n+1} + \frac{\mu}{\lambda+\mu}p_{n-1}. \]
    \item La récurrence est $\lambda p_{n+1} - (\lambda+\mu)p_n + \mu p_{n-1} = 0$. On peut la réécrire $(\lambda p_{n+1} - \lambda p_n) = (\mu p_n - \mu p_{n-1})$. Soit $d_n = p_n - p_{n-1}$. Alors $\lambda d_{n+1} = \mu d_n$, donc $d_{n+1} = (\mu/\lambda)d_n$. C'est une suite géométrique. Si $\lambda < \mu$, alors $\mu/\lambda > 1$. Pour que la suite $(p_n)$ (qui est une probabilité) reste bornée par 1, il faut que la raison de la suite géométrique soit telle que la somme converge, ce qui impose $d_1=0$, et donc $d_n=0$ pour tout $n$. Donc $p_n = p_0 = 1$. La chaîne est récurrente. Si $\lambda \ge \mu$, il existe une solution non-constante, et la probabilité d'atteindre 0 est inférieure à 1.
    \item \textbf{TP-} L'algorithme de Gillespie est implémenté dans le script.
    \item \textbf{TP-} Les simulations montreront :
    \begin{itemize}
        \item $\lambda > \mu$ : La file d'attente est instable, le nombre de clients croît en moyenne linéairement.
        \item $\lambda = \mu$ : Cas critique, la file est instable (récurrente nulle), elle peut atteindre des valeurs très élevées.
        \item $\lambda < \mu$ : La file est stable, le nombre de clients fluctue autour d'une moyenne.
    \end{itemize}
    \item \textbf{TP-} Quand $\lambda > \mu$, le taux d'arrivée est supérieur au taux de service. Le nombre de clients $W_t$ devrait croître en moyenne à une vitesse de $\lambda - \mu$. Le graphe de $W_t/t$ (ou $Z_n/T_n$) devrait converger vers cette valeur.
    \item \textbf{TP-} Pour le cas stable $\lambda < \mu$, on doit vérifier la distribution stationnaire $\pi_k = (1-\rho)\rho^k$ où $\rho=\lambda/\mu$. Pour cela, on ne peut pas utiliser l'histogramme de la chaîne de sauts $(Z_n)$. On doit calculer la proportion de \textit{temps} passé dans chaque état. On simule sur un temps long $T_{max}$ et pour chaque état $k$, on somme les durées de séjour $T_{n+1}-T_n$ où $Z_n=k$. La proportion est cette somme divisée par $T_{max}$.
    \item  \textbf{TP-} Un temps d'inactivité est une période où la file est vide. Cela commence quand $Z_n$ passe de 1 à 0 et se termine au saut suivant (qui est forcément une arrivée). Le temps d'attente pour cette arrivée est une variable $\mathcal{E}(\lambda)$. La moyenne de ces temps d'inactivité doit donc converger vers $1/\lambda$. C'est cohérent, car lorsque le serveur est libre, seules les arrivées comptent.
\end{enumerate}
\end{solution}

\begin{exercice}[Processus de Poisson dans la vie réelle]
On rappelle qu'une loi de Poisson de paramètre $\lambda$ prend ses valeurs dans $\mathbb{N}$ et que si $X$ suit une loi de Poisson,
$$\mathbb{P}[X=k]=e^{-\lambda}\frac{\lambda^k}{k!} .$$
Dans les situations ci-dessous, on pourra supposer poissonnienne la loi de survenance des évènements.
\begin{enumerate}
\item Un magasin reçoit 3 réclamations en moyenne par jour. A partir de cette observation, quelle valeur donneriez vous à $\lambda$ ?\\
Calculer alors la probabilité pour que le premier lundi du mois prochain soient enregistrées 0 réclamation; 2 réclamations; puis enfin plus de 4 réclamations.
\item La probabilité pour une ampoule électrique de claquer à son premier allumage est de 0,01. Sur un groupe de 100 ampoules, quelle est la probabilité d'observer 0 claquage; 1 claquage; plus de 2 claquages.
\item Sur une autoroute, il y a en moyenne un accident par semaine. Une semaine, il y en a 4.
Quelle est la probabilité de cet événement ?
\item Le statisticien anglais Clarke a divisé Londres en 576 rectangles et compté les chutes de bombes dans ces rectangles durant la 2ème guerre mondiale 1939-1945.
Il a trouvé :
\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Nombre de bombes & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline
Nombres de rectangles & 229 & 211 & 93 & 35 & 7 & 1 \\
\hline
\end{tabular}
\end{figure}
Calculer la moyenne L du nombre de bombes par rectangle. \\
Comparer la distribution réelle à la distribution résultant de l'application de la loi de Poisson de même moyenne.

\item Von Bortkiewicz a étudié le nombre de morts par ruade de cheval dans l'armée prussienne de 1875 à 1894 dans 200 corps de cavalerie : pendant 20 ans, il a étudié 10 corps de cavalerie par an.
\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Nombre de morts par an & 0 & 1 & 2 & 3 & 4 \\
\hline
Nombres de corps de cavalerie & 109 & 65 & 22 & 3 & 1  \\
\hline
\end{tabular}
\end{figure}
Calculer la moyenne du nombre de morts par corps de cavalerie. Comparer la distribution réelle à la distribution résultant de l'application de la loi de Poisson de paramètre L.
\end{enumerate}
\end{exercice}

\begin{solution}
\begin{enumerate}
    \item On observe une moyenne de 3 réclamations par jour, on pose donc $\lambda=3$. Soit $X \sim \text{Poisson}(3)$.
    \begin{itemize}
        \item $\P(X=0) = e^{-3} \frac{3^0}{0!} = e^{-3} \approx 0.0498$.
        \item $\P(X=2) = e^{-3} \frac{3^2}{2!} = 4.5 e^{-3} \approx 0.2240$.
        \item $\P(X>4) = 1 - \P(X \le 4) = 1 - e^{-3}(\frac{3^0}{0!} + \frac{3^1}{1!} + \frac{3^2}{2!} + \frac{3^3}{3!} + \frac{3^4}{4!}) = 1 - e^{-3}(1+3+4.5+4.5+3.375) = 1 - 16.375 e^{-3} \approx 0.1847$.
    \end{itemize}
    \item C'est un cas de figure pour l'approximation de la loi binomiale par une loi de Poisson. On a $n=100$ (grand) et $p=0.01$ (petit). On peut approcher $X \sim \mathcal{B}(100, 0.01)$ par $Y \sim \text{Poisson}(\lambda=np=1)$.
    \begin{itemize}
        \item $\P(Y=0) = e^{-1} \approx 0.3679$.
        \item $\P(Y=1) = e^{-1} \frac{1^1}{1!} = e^{-1} \approx 0.3679$.
        \item $\P(Y>2) = 1 - \P(Y \le 2) = 1 - e^{-1}(1+1+\frac{1}{2}) = 1-2.5e^{-1} \approx 0.0803$.
    \end{itemize}
    \item On a $\lambda=1$ accident par semaine. On veut calculer $\P(X=4)$ pour $X \sim \text{Poisson}(1)$.
    \[ \P(X=4) = e^{-1} \frac{1^4}{4!} = \frac{e^{-1}}{24} \approx 0.0153. \]
    C'est un événement rare, mais pas impossible.
    \item On calcule la moyenne observée :
    Nombre total de bombes = $(0 \times 229) + (1 \times 211) + (2 \times 93) + (3 \times 35) + (4 \times 7) + (5 \times 1) = 0 + 211 + 186 + 105 + 28 + 5 = 535$.
    Nombre total de rectangles = $229+211+93+35+7+1 = 576$.
    Moyenne $L = \frac{535}{576} \approx 0.9288$.
    On compare les fréquences observées aux fréquences théoriques pour une loi de Poisson de paramètre $L=0.9288$. Effectif théorique pour $k$ bombes = $576 \times \P(X=k)$.
    \begin{itemize}
        \item $k=0: 576 \times e^{-L} \approx 226.7$ (Observé: 229)
        \item $k=1: 576 \times L e^{-L} \approx 210.5$ (Observé: 211)
        \item $k=2: 576 \times \frac{L^2}{2} e^{-L} \approx 97.7$ (Observé: 93)
    \end{itemize}
    L'adéquation est remarquablement bonne.
    \item On calcule la moyenne observée :
    Nombre total de morts = $(0 \times 109) + (1 \times 65) + (2 \times 22) + (3 \times 3) + (4 \times 1) = 0+65+44+9+4 = 122$.
    Nombre total d'observations (corps $\times$ années) = $109+65+22+3+1 = 200$.
    Moyenne $L = \frac{122}{200} = 0.61$.
    On compare aux fréquences théoriques pour une loi de Poisson de paramètre $L=0.61$. Effectif théorique pour $k$ morts = $200 \times \P(X=k)$.
    \begin{itemize}
        \item $k=0: 200 \times e^{-0.61} \approx 108.7$ (Observé: 109)
        \item $k=1: 200 \times 0.61 e^{-0.61} \approx 66.3$ (Observé: 65)
        \item $k=2: 200 \times \frac{0.61^2}{2} e^{-0.61} \approx 20.2$ (Observé: 22)
    \end{itemize}
    L'adéquation est, là aussi, excellente.
\end{enumerate}
\end{solution}

\end{document}
